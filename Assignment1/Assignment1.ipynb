{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/janchorowski/dl_uwr/blob/summer2020/Assignment1/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAiECkYEaZn1"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "## Important notes\n",
    "**Submission deadline:**\n",
    "* **Thursday, 12.03.2020**\n",
    "\n",
    "**Points: 13 + 2bp**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bb0zN1GvapSt"
   },
   "source": [
    "This assignment is meant to test your skills in course pre-requisites:  Scientific Python programming and  Machine Learning. If it is hard, I strongly advise you to drop the course.\n",
    "\n",
    "Please use GitHub’s [pull requests](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests) and issues to send corrections!\n",
    "\n",
    "You can solve the assignment in any system you like, but we encourage you to try out [Google Colab](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIBf-IF_ahTI"
   },
   "source": [
    "## Assignment text\n",
    "1. **[1p]** Download data competition from a Kaggle competition on sentiment prediction from [[https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data)].  Keep only full sentences, i.e. for each `SenteceId` keep only the entry with the lowest `PhraseId`.  Use first 7000 sentences as a `train set` and the remaining 1529 sentences as the `test set`. \n",
    "\n",
    "2. **[1p]** Prepare the data for logistic regression:\n",
    "\tMap the sentiment scores $0,1,2,3,4$ to a probability of the sentence being by setting $p(\\textrm{positive}) = \\textrm{sentiment}/4$.\n",
    "\tBuild a dictionary of at most 20000 most frequent words.\n",
    "\n",
    "3. **[3p]** Treat each document as a bag of words. e.g. if the vocabulary is \n",
    "\t```\n",
    "\t0: the\n",
    "\t1: good\n",
    "\t2: movie\n",
    "\t3: is\n",
    "\t4: not\n",
    "\t5: a\n",
    "\t6: funny\n",
    "\t```\n",
    "\tThen the encodings can be:\n",
    "\t```\n",
    "\tgood:                           [0,1,0,0,0,0,0]\n",
    "\tnot good:                       [0,1,0,0,1,0,0] \n",
    "\tthe movie is not a funny movie: [1,0,2,1,1,1,1]\n",
    "\t```\n",
    "    Train a logistic regression model to predict the sentiment. Compute the correlation between the predicted probabilities and the sentiment. Record the most positive and negative words.\n",
    "    Please note that in this model each word gets its sentiment parameter $S_w$ and the score for a sentence is \n",
    "    $$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}S_w$$\n",
    "\n",
    "4. **[3p]** Now prepare an encoding in which negation flips the sign of the following words. For instance for our vocabulary the encodings become:\n",
    "\t```\n",
    "\tgood:                           [0,1,0,0,0,0,0]\n",
    "\tnot good:                       [0,-1,0,0,1,0,0]\n",
    "\tnot not good:                   [0,1,0,0,0,0,0]\n",
    "\tthe movie is not a funny movie: [1,0,0,1,1,-1,-1]\n",
    "\t```\n",
    "\tFor best results, you will probably need to construct a list of negative words.\n",
    "\t\n",
    "\tAgain train a logistic regression classifier and compare the results to the Bag of Words approach.\n",
    "\t\n",
    "\tPlease note that this model still maintains a single parameter for each word, but now the sentence score is\n",
    "\t$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}-1^{\\text{count of negations preceeding }w}S_w$$\n",
    "\n",
    "5. **[5p]** Now also consider emphasizing words such as `very`. They can boost (multiply by a constant >1) the following words.\n",
    "\tImplement learning the modifying multiplier for negation and for emphasis. One way to do this is to introduce a model which has:\n",
    "\t- two modifiers, $N$ for negation and $E$ for emphasis\n",
    "\t- a sentiment score $S_w$ for each word \n",
    "And score each sentence as:\n",
    "$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}N^{\\text{\\#negs prec. }w}E^{\\text{\\#emphs prec. }w}S_w$$\n",
    "\n",
    "You will need to implement a custom logistic regression model to support it.\n",
    "\n",
    "6. **[2pb]** Propose, implement, and evaluate an extension to the above model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import re, string\n",
    "import scipy.optimize as sopt\n",
    "from collections import Counter,OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = pd.DataFrame(columns=['PhraseId', 'SentenceId', 'Phrase', 'Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by = ['SentenceId', 'PhraseId'], ascending = (True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_sent_id = 0\n",
    "counter = 0\n",
    "for index, row in df.iterrows():\n",
    "    if last_sent_id!=row['SentenceId']:\n",
    "        counter+=1\n",
    "        df_copy = df_copy.append(row, ignore_index = True)\n",
    "        last_sent_id = row['SentenceId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157</td>\n",
       "      <td>5</td>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PhraseId SentenceId                                             Phrase  \\\n",
       "0        1          1  A series of escapades demonstrating the adage ...   \n",
       "1       64          2  This quiet , introspective and entertaining in...   \n",
       "2       82          3  Even fans of Ismail Merchant 's work , I suspe...   \n",
       "3      117          4  A positively thrilling combination of ethnogra...   \n",
       "4      157          5  Aggressive self-glorification and a manipulati...   \n",
       "\n",
       "  Sentiment  \n",
       "0         1  \n",
       "1         4  \n",
       "2         1  \n",
       "3         3  \n",
       "4         1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_copy.iloc[:7000]\n",
    "test_df = df_copy[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = np.zeros(7000)\n",
    "for i, row in train_df.iterrows():\n",
    "    sentiment[i] = row['Sentiment']/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>130088</td>\n",
       "      <td>7007</td>\n",
       "      <td>Snoots will no doubt rally to its cause , trot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>130131</td>\n",
       "      <td>7008</td>\n",
       "      <td>It 's better suited for the history or biograp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>130161</td>\n",
       "      <td>7009</td>\n",
       "      <td>Buries an interesting storyline</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>130181</td>\n",
       "      <td>7010</td>\n",
       "      <td>This one is a few bits funnier than Malle 's d...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>130214</td>\n",
       "      <td>7011</td>\n",
       "      <td>The film has several strong performances .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PhraseId SentenceId                                             Phrase  \\\n",
       "6995   130088       7007  Snoots will no doubt rally to its cause , trot...   \n",
       "6996   130131       7008  It 's better suited for the history or biograp...   \n",
       "6997   130161       7009                    Buries an interesting storyline   \n",
       "6998   130181       7010  This one is a few bits funnier than Malle 's d...   \n",
       "6999   130214       7011         The film has several strong performances .   \n",
       "\n",
       "     Sentiment  \n",
       "6995         1  \n",
       "6996         1  \n",
       "6997         2  \n",
       "6998         3  \n",
       "6999         3  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "for i, row in train_df.iterrows():\n",
    "    #pattern = re.compile('([\\w]|)')\n",
    "    phrase = row['Phrase']\n",
    "    phrase.translate(str.maketrans('', '', string.punctuation))\n",
    "    word = \"\"\n",
    "    for s in phrase:\n",
    "        if s == ' ':\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            if word not in words:\n",
    "                words[word]=0\n",
    "            words[word]+=1\n",
    "            word = \"\"\n",
    "        else:\n",
    "            word += s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted(words.items(), key=lambda x: x[1])\n",
    "words = words[-2000:]\n",
    "words = dict(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for key in words:\n",
    "    vocab[key] = counter\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gritty': 0,\n",
       " 'vincent': 1,\n",
       " 'honestly': 2,\n",
       " 'bloody': 3,\n",
       " 'chabrol': 4,\n",
       " 'gently': 5,\n",
       " 'beneath': 6,\n",
       " 'shakespeare': 7,\n",
       " 'tragedies': 8,\n",
       " 'common': 9,\n",
       " 'screenwriters': 10,\n",
       " 'technical': 11,\n",
       " 'resonant': 12,\n",
       " 'affection': 13,\n",
       " 'victim': 14,\n",
       " 'stop': 15,\n",
       " 'returns': 16,\n",
       " 'innocence': 17,\n",
       " 'succeed': 18,\n",
       " 'element': 19,\n",
       " 'potent': 20,\n",
       " 'revelatory': 21,\n",
       " 'martha': 22,\n",
       " 'refreshingly': 23,\n",
       " 'dvd': 24,\n",
       " 'open': 25,\n",
       " 'clumsy': 26,\n",
       " 'legged': 27,\n",
       " 'freaks': 28,\n",
       " 'romp': 29,\n",
       " 'bigger': 30,\n",
       " 'explosions': 31,\n",
       " 'feature-length': 32,\n",
       " 'uplifting': 33,\n",
       " 'graphic': 34,\n",
       " 'smug': 35,\n",
       " 'superior': 36,\n",
       " 'heavy-handed': 37,\n",
       " 'smartly': 38,\n",
       " 'eccentric': 39,\n",
       " 'week': 40,\n",
       " 'recycled': 41,\n",
       " 'teenage': 42,\n",
       " 'parable': 43,\n",
       " 'keeping': 44,\n",
       " 'public': 45,\n",
       " 'computer': 46,\n",
       " 'increasingly': 47,\n",
       " 'woo': 48,\n",
       " 'pair': 49,\n",
       " 'photographed': 50,\n",
       " 'staged': 51,\n",
       " 'good-natured': 52,\n",
       " 'incoherent': 53,\n",
       " 'mean-spirited': 54,\n",
       " 'biting': 55,\n",
       " 'ticket': 56,\n",
       " 'clarity': 57,\n",
       " 'logic': 58,\n",
       " 'false': 59,\n",
       " 'hey': 60,\n",
       " 'stand-up': 61,\n",
       " 'spare': 62,\n",
       " 'stops': 63,\n",
       " 'grandeur': 64,\n",
       " 'copy': 65,\n",
       " 'characterizations': 66,\n",
       " 'fable': 67,\n",
       " 'tasty': 68,\n",
       " 'fight': 69,\n",
       " 'pulls': 70,\n",
       " 'hill': 71,\n",
       " 'secretary': 72,\n",
       " 'candy': 73,\n",
       " 'laughing': 74,\n",
       " 'behavior': 75,\n",
       " 'rip-off': 76,\n",
       " 'startling': 77,\n",
       " 'obnoxious': 78,\n",
       " 'somber': 79,\n",
       " 'ramsay': 80,\n",
       " 'seven': 81,\n",
       " 'search': 82,\n",
       " 'standards': 83,\n",
       " 'essence': 84,\n",
       " 'started': 85,\n",
       " 'handled': 86,\n",
       " 'santa': 87,\n",
       " 'encounter': 88,\n",
       " 'mouth': 89,\n",
       " 'stunts': 90,\n",
       " 'messages': 91,\n",
       " 'forgotten': 92,\n",
       " 'follows': 93,\n",
       " 'stuart': 94,\n",
       " 'count': 95,\n",
       " 'delicately': 96,\n",
       " 'evocative': 97,\n",
       " 'levels': 98,\n",
       " 'giant': 99,\n",
       " 'design': 100,\n",
       " 'lyrical': 101,\n",
       " 'enjoyed': 102,\n",
       " 'slowly': 103,\n",
       " 'jackie': 104,\n",
       " 'younger': 105,\n",
       " 'inner': 106,\n",
       " 'due': 107,\n",
       " 'halloween': 108,\n",
       " 'ahead': 109,\n",
       " 'darker': 110,\n",
       " 'cloying': 111,\n",
       " 'positive': 112,\n",
       " 'talents': 113,\n",
       " 'unusual': 114,\n",
       " 'desperate': 115,\n",
       " 'leaving': 116,\n",
       " 'mature': 117,\n",
       " 'haunting': 118,\n",
       " 'spectacular': 119,\n",
       " 'determined': 120,\n",
       " 'current': 121,\n",
       " 'came': 122,\n",
       " 'scores': 123,\n",
       " 'camp': 124,\n",
       " 'follow': 125,\n",
       " 'promising': 126,\n",
       " 'holocaust': 127,\n",
       " 'parts': 128,\n",
       " 'obviously': 129,\n",
       " 'free': 130,\n",
       " 'muddled': 131,\n",
       " 'hugely': 132,\n",
       " 'imagery': 133,\n",
       " 'dance': 134,\n",
       " 'sparkling': 135,\n",
       " 'sand': 136,\n",
       " 'acceptable': 137,\n",
       " 'herself': 138,\n",
       " 'capture': 139,\n",
       " 'curious': 140,\n",
       " 'attractive': 141,\n",
       " 'consider': 142,\n",
       " 'saga': 143,\n",
       " 'technology': 144,\n",
       " 'sweetly': 145,\n",
       " 'skip': 146,\n",
       " 'admire': 147,\n",
       " 'empire': 148,\n",
       " 'reasonably': 149,\n",
       " 'amy': 150,\n",
       " 'lousy': 151,\n",
       " 'damage': 152,\n",
       " 'packed': 153,\n",
       " 'mild': 154,\n",
       " 'slap': 155,\n",
       " 'scenario': 156,\n",
       " 'distance': 157,\n",
       " 'stilted': 158,\n",
       " 'rhythm': 159,\n",
       " 'helps': 160,\n",
       " 'songs': 161,\n",
       " 'rendered': 162,\n",
       " 'drags': 163,\n",
       " 'breaks': 164,\n",
       " 'woody': 165,\n",
       " 'hands': 166,\n",
       " \"'70s\": 167,\n",
       " 'desperation': 168,\n",
       " 'cheesy': 169,\n",
       " 'respectable': 170,\n",
       " 'hybrid': 171,\n",
       " 'larger': 172,\n",
       " 'starring': 173,\n",
       " 'delightfully': 174,\n",
       " 'faithful': 175,\n",
       " 'third': 176,\n",
       " 'monsters': 177,\n",
       " 'rewards': 178,\n",
       " 'carry': 179,\n",
       " 'sitcom': 180,\n",
       " 'artifice': 181,\n",
       " 'required': 182,\n",
       " 'sees': 183,\n",
       " 'fluff': 184,\n",
       " 'continues': 185,\n",
       " 'showing': 186,\n",
       " 'understated': 187,\n",
       " 'shoot': 188,\n",
       " 'wow': 189,\n",
       " 'similar': 190,\n",
       " 'pedestrian': 191,\n",
       " 'rise': 192,\n",
       " 'finale': 193,\n",
       " 'diesel': 194,\n",
       " 'boasts': 195,\n",
       " 'vital': 196,\n",
       " 'notorious': 197,\n",
       " 'life-affirming': 198,\n",
       " 'imaginative': 199,\n",
       " 'bartleby': 200,\n",
       " 'category': 201,\n",
       " 'bringing': 202,\n",
       " 'meant': 203,\n",
       " 'psychology': 204,\n",
       " 'lies': 205,\n",
       " 'excess': 206,\n",
       " 'elaborate': 207,\n",
       " 'poignancy': 208,\n",
       " 'scorsese': 209,\n",
       " 'k-19': 210,\n",
       " 'sour': 211,\n",
       " 'no.': 212,\n",
       " 'vibrant': 213,\n",
       " 'ponderous': 214,\n",
       " 'creativity': 215,\n",
       " 'accessible': 216,\n",
       " 'hackneyed': 217,\n",
       " 'compassion': 218,\n",
       " 'constantly': 219,\n",
       " 'promises': 220,\n",
       " 'harris': 221,\n",
       " 'nicholas': 222,\n",
       " 'service': 223,\n",
       " 'soldiers': 224,\n",
       " 'kapur': 225,\n",
       " 'deserve': 226,\n",
       " 'english': 227,\n",
       " 'die': 228,\n",
       " 'smoochy': 229,\n",
       " 'heartbreaking': 230,\n",
       " 'warning': 231,\n",
       " 'beginning': 232,\n",
       " 'says': 233,\n",
       " 'victims': 234,\n",
       " 'eddie': 235,\n",
       " 'books': 236,\n",
       " 'after-school': 237,\n",
       " 'authority': 238,\n",
       " 'simultaneously': 239,\n",
       " 'development': 240,\n",
       " 'timing': 241,\n",
       " 'unfaithful': 242,\n",
       " 'touches': 243,\n",
       " 'cultures': 244,\n",
       " 'contains': 245,\n",
       " 'sentiment': 246,\n",
       " 'choices': 247,\n",
       " \"'em\": 248,\n",
       " 'cletis': 249,\n",
       " 'child': 250,\n",
       " 'stealing': 251,\n",
       " 'brother': 252,\n",
       " 'saturday': 253,\n",
       " 'pathetic': 254,\n",
       " 'stuck': 255,\n",
       " 'lackluster': 256,\n",
       " 'trappings': 257,\n",
       " 'silliness': 258,\n",
       " 'lady': 259,\n",
       " 'pain': 260,\n",
       " 'office': 261,\n",
       " 'bold': 262,\n",
       " 'tales': 263,\n",
       " 'kissinger': 264,\n",
       " 'importance': 265,\n",
       " 'dogs': 266,\n",
       " 'charmer': 267,\n",
       " 'aims': 268,\n",
       " 'avoid': 269,\n",
       " 'horrifying': 270,\n",
       " 'cliche': 271,\n",
       " 'pat': 272,\n",
       " 'endless': 273,\n",
       " 'designed': 274,\n",
       " 'roll': 275,\n",
       " 'delicate': 276,\n",
       " 'pianist': 277,\n",
       " 'quick': 278,\n",
       " 'curiosity': 279,\n",
       " 'report': 280,\n",
       " 'tsai': 281,\n",
       " 'equivalent': 282,\n",
       " 'ask': 283,\n",
       " 'achievement': 284,\n",
       " 'record': 285,\n",
       " 'masterful': 286,\n",
       " 'appreciate': 287,\n",
       " 'russian': 288,\n",
       " 'land': 289,\n",
       " 'cause': 290,\n",
       " 'carries': 291,\n",
       " 'efforts': 292,\n",
       " 'madonna': 293,\n",
       " 'dangerous': 294,\n",
       " 'key': 295,\n",
       " 'partly': 296,\n",
       " 'uninspired': 297,\n",
       " 'song': 298,\n",
       " 'example': 299,\n",
       " 'scratch': 300,\n",
       " 'adolescent': 301,\n",
       " 'hopeful': 302,\n",
       " 'flair': 303,\n",
       " 'yarn': 304,\n",
       " 'punch': 305,\n",
       " 'hits': 306,\n",
       " 'impression': 307,\n",
       " 'catch': 308,\n",
       " 'enthusiasm': 309,\n",
       " 'blend': 310,\n",
       " 'rhythms': 311,\n",
       " 'news': 312,\n",
       " 'humour': 313,\n",
       " 'narc': 314,\n",
       " 'cynical': 315,\n",
       " 'proceedings': 316,\n",
       " 'handsome': 317,\n",
       " 'admit': 318,\n",
       " 'halfway': 319,\n",
       " 'preachy': 320,\n",
       " 'walk': 321,\n",
       " 'directing': 322,\n",
       " 'moves': 323,\n",
       " 'finding': 324,\n",
       " 'broken': 325,\n",
       " 'revealing': 326,\n",
       " 'asks': 327,\n",
       " 'spoof': 328,\n",
       " 'buy': 329,\n",
       " 'matters': 330,\n",
       " 'foreign': 331,\n",
       " 'band': 332,\n",
       " 'mere': 333,\n",
       " 'hip-hop': 334,\n",
       " 'flashy': 335,\n",
       " 'laughter': 336,\n",
       " 'easier': 337,\n",
       " 'ode': 338,\n",
       " 'artificial': 339,\n",
       " 'davis': 340,\n",
       " 'ritchie': 341,\n",
       " 'feel-good': 342,\n",
       " 'sappy': 343,\n",
       " 'indeed': 344,\n",
       " 'subtlety': 345,\n",
       " 'crimes': 346,\n",
       " 'speaking': 347,\n",
       " 'hearts': 348,\n",
       " 'onscreen': 349,\n",
       " 'wry': 350,\n",
       " 'unpleasant': 351,\n",
       " 'sick': 352,\n",
       " 'dignity': 353,\n",
       " 'despair': 354,\n",
       " 'schmidt': 355,\n",
       " 'twisted': 356,\n",
       " 'fair': 357,\n",
       " 'brothers': 358,\n",
       " 'van': 359,\n",
       " 'families': 360,\n",
       " 'straightforward': 361,\n",
       " 'event': 362,\n",
       " 'lightweight': 363,\n",
       " 'barry': 364,\n",
       " 'throws': 365,\n",
       " 'thought-provoking': 366,\n",
       " 'meandering': 367,\n",
       " 'color': 368,\n",
       " 'treasure': 369,\n",
       " 'japanese': 370,\n",
       " 'cult': 371,\n",
       " 'maintains': 372,\n",
       " 'iranian': 373,\n",
       " 'fill': 374,\n",
       " 'playful': 375,\n",
       " 'market': 376,\n",
       " 'retread': 377,\n",
       " 'detailed': 378,\n",
       " 'bittersweet': 379,\n",
       " 'pity': 380,\n",
       " 'suffering': 381,\n",
       " 'limited': 382,\n",
       " 'loves': 383,\n",
       " 'loved': 384,\n",
       " 'patience': 385,\n",
       " 'harmless': 386,\n",
       " 'admirable': 387,\n",
       " 'glass': 388,\n",
       " 'teens': 389,\n",
       " 'comedian': 390,\n",
       " 'collection': 391,\n",
       " 'abstract': 392,\n",
       " 'persona': 393,\n",
       " 'acts': 394,\n",
       " 'unintentionally': 395,\n",
       " 'parody': 396,\n",
       " 'deliver': 397,\n",
       " 'kaufman': 398,\n",
       " 'artistic': 399,\n",
       " 'credible': 400,\n",
       " 'guns': 401,\n",
       " 'secrets': 402,\n",
       " 'damn': 403,\n",
       " 'including': 404,\n",
       " 'supporting': 405,\n",
       " 'total': 406,\n",
       " 'stomach': 407,\n",
       " 'ridiculous': 408,\n",
       " 'capable': 409,\n",
       " 'answers': 410,\n",
       " 'deftly': 411,\n",
       " 'group': 412,\n",
       " 'manage': 413,\n",
       " 'digital': 414,\n",
       " 'belongs': 415,\n",
       " 'relatively': 416,\n",
       " 'traditional': 417,\n",
       " 'buoyant': 418,\n",
       " 'profound': 419,\n",
       " 'italian': 420,\n",
       " 'justify': 421,\n",
       " 'front': 422,\n",
       " 'revolution': 423,\n",
       " 'biggest': 424,\n",
       " 'sade': 425,\n",
       " 'realize': 426,\n",
       " 'mistake': 427,\n",
       " 'onto': 428,\n",
       " 'belly': 429,\n",
       " 'focus': 430,\n",
       " 'bitter': 431,\n",
       " 'forgive': 432,\n",
       " 'terrifying': 433,\n",
       " 'observations': 434,\n",
       " 'richly': 435,\n",
       " 'exploitation': 436,\n",
       " 'overwrought': 437,\n",
       " 'holes': 438,\n",
       " 'cautionary': 439,\n",
       " 'farce': 440,\n",
       " 'gradually': 441,\n",
       " 'stands': 442,\n",
       " 'folks': 443,\n",
       " 'burns': 444,\n",
       " 'uncompromising': 445,\n",
       " 'devastating': 446,\n",
       " 'antwone': 447,\n",
       " 'enigma': 448,\n",
       " 'smarter': 449,\n",
       " 'executed': 450,\n",
       " 'nevertheless': 451,\n",
       " 'visceral': 452,\n",
       " 'outrageous': 453,\n",
       " 'sugar': 454,\n",
       " 'chinese': 455,\n",
       " 'well-meaning': 456,\n",
       " 'expectations': 457,\n",
       " 'theaters': 458,\n",
       " 'well-acted': 459,\n",
       " 'bright': 460,\n",
       " 'snow': 461,\n",
       " 'dreams': 462,\n",
       " 'mile': 463,\n",
       " 'modest': 464,\n",
       " 'inspired': 465,\n",
       " 'inept': 466,\n",
       " 'offering': 467,\n",
       " 'party': 468,\n",
       " 'early': 469,\n",
       " 'makers': 470,\n",
       " 'fatal': 471,\n",
       " 'totally': 472,\n",
       " 'team': 473,\n",
       " 'reno': 474,\n",
       " 'tender': 475,\n",
       " 'struggle': 476,\n",
       " 'credits': 477,\n",
       " 'list': 478,\n",
       " 'courage': 479,\n",
       " 'slice': 480,\n",
       " 'bottom': 481,\n",
       " 'writer\\\\/director': 482,\n",
       " 'emotion': 483,\n",
       " 'analyze': 484,\n",
       " 'sequels': 485,\n",
       " 'changing': 486,\n",
       " 'science': 487,\n",
       " 'lawrence': 488,\n",
       " 'within': 489,\n",
       " 'polanski': 490,\n",
       " 'spiritual': 491,\n",
       " 'prove': 492,\n",
       " 'unlikely': 493,\n",
       " 'winds': 494,\n",
       " 'precious': 495,\n",
       " 'affecting': 496,\n",
       " 'natural': 497,\n",
       " 'rush': 498,\n",
       " 'intoxicating': 499,\n",
       " 'homage': 500,\n",
       " 'b-movie': 501,\n",
       " 'wears': 502,\n",
       " 'bunch': 503,\n",
       " 'played': 504,\n",
       " 'well-made': 505,\n",
       " 'wallace': 506,\n",
       " 'poetic': 507,\n",
       " 'realism': 508,\n",
       " 'lines': 509,\n",
       " 'reign': 510,\n",
       " 'degree': 511,\n",
       " 'humorous': 512,\n",
       " 'revenge': 513,\n",
       " 'unfolds': 514,\n",
       " 'jason': 515,\n",
       " 'serves': 516,\n",
       " 'tiresome': 517,\n",
       " 'typical': 518,\n",
       " 'killer': 519,\n",
       " 'superficial': 520,\n",
       " 'moviemaking': 521,\n",
       " 'roger': 522,\n",
       " 'holiday': 523,\n",
       " 'bag': 524,\n",
       " 'below': 525,\n",
       " 'adds': 526,\n",
       " 'various': 527,\n",
       " 'rises': 528,\n",
       " 'joyous': 529,\n",
       " 'present': 530,\n",
       " 'apparent': 531,\n",
       " 'loss': 532,\n",
       " 'sounds': 533,\n",
       " 'sadly': 534,\n",
       " 'austin': 535,\n",
       " 'inspiring': 536,\n",
       " 'spark': 537,\n",
       " 'particular': 538,\n",
       " 'using': 539,\n",
       " 'saying': 540,\n",
       " 'intimate': 541,\n",
       " 'irritating': 542,\n",
       " 'mainly': 543,\n",
       " 'joke': 544,\n",
       " 'target': 545,\n",
       " 'funniest': 546,\n",
       " 'features': 547,\n",
       " 'superb': 548,\n",
       " 'miller': 549,\n",
       " 'nostalgia': 550,\n",
       " 'musical': 551,\n",
       " 'paul': 552,\n",
       " 'strictly': 553,\n",
       " 'community': 554,\n",
       " 'experiences': 555,\n",
       " 'charms': 556,\n",
       " 'grand': 557,\n",
       " 'kevin': 558,\n",
       " 'badly': 559,\n",
       " 'escape': 560,\n",
       " 'self-conscious': 561,\n",
       " 'trifle': 562,\n",
       " 'screenwriting': 563,\n",
       " 'low-key': 564,\n",
       " 'decades': 565,\n",
       " 'heard': 566,\n",
       " 'voice': 567,\n",
       " 'sign': 568,\n",
       " 'pleasures': 569,\n",
       " 'stunning': 570,\n",
       " 'secret': 571,\n",
       " 'difference': 572,\n",
       " 'potentially': 573,\n",
       " 'space': 574,\n",
       " 'excitement': 575,\n",
       " 'extraordinary': 576,\n",
       " 'pick': 577,\n",
       " 'detail': 578,\n",
       " 'lesson': 579,\n",
       " 'actresses': 580,\n",
       " 'dazzling': 581,\n",
       " 'sympathy': 582,\n",
       " 'birthday': 583,\n",
       " 'predictably': 584,\n",
       " 'sports': 585,\n",
       " 'howard': 586,\n",
       " 'stage': 587,\n",
       " 'politics': 588,\n",
       " 'check': 589,\n",
       " 'frustrating': 590,\n",
       " 'godard': 591,\n",
       " 'theatre': 592,\n",
       " 'god': 593,\n",
       " 'low-budget': 594,\n",
       " 'disaster': 595,\n",
       " 'deadly': 596,\n",
       " 'real-life': 597,\n",
       " 'zone': 598,\n",
       " 'reminds': 599,\n",
       " 'nicholson': 600,\n",
       " 'heartwarming': 601,\n",
       " 'constructed': 602,\n",
       " 'fierce': 603,\n",
       " 'studio': 604,\n",
       " 'frida': 605,\n",
       " 'realized': 606,\n",
       " 'dragon': 607,\n",
       " 'lame': 608,\n",
       " 'incredibly': 609,\n",
       " 'rewarding': 610,\n",
       " 'earth': 611,\n",
       " 'fear': 612,\n",
       " 'surely': 613,\n",
       " 'necessary': 614,\n",
       " 'reach': 615,\n",
       " 'son': 616,\n",
       " 'aspects': 617,\n",
       " 'cruel': 618,\n",
       " 'ryan': 619,\n",
       " 'angst': 620,\n",
       " 'exploration': 621,\n",
       " 'pack': 622,\n",
       " 'broomfield': 623,\n",
       " 'ludicrous': 624,\n",
       " 'excuse': 625,\n",
       " 'grows': 626,\n",
       " 'becoming': 627,\n",
       " 'elegant': 628,\n",
       " 'inspiration': 629,\n",
       " 'missed': 630,\n",
       " 'type': 631,\n",
       " 'choppy': 632,\n",
       " 'vs.': 633,\n",
       " 'fashion': 634,\n",
       " 'glimpse': 635,\n",
       " 'repetitive': 636,\n",
       " 'adult': 637,\n",
       " 'spent': 638,\n",
       " 'holds': 639,\n",
       " 'greek': 640,\n",
       " 'return': 641,\n",
       " 'sides': 642,\n",
       " 'testament': 643,\n",
       " 'outside': 644,\n",
       " 'banal': 645,\n",
       " 'parker': 646,\n",
       " 'hoffman': 647,\n",
       " 'plodding': 648,\n",
       " 'source': 649,\n",
       " 'perspective': 650,\n",
       " 'meaning': 651,\n",
       " 'games': 652,\n",
       " 'james': 653,\n",
       " 'delivered': 654,\n",
       " 'downright': 655,\n",
       " 'skin': 656,\n",
       " 'results': 657,\n",
       " 'shyamalan': 658,\n",
       " 'theme': 659,\n",
       " 'yourself': 660,\n",
       " 'stirring': 661,\n",
       " 'sequence': 662,\n",
       " 'spider-man': 663,\n",
       " 'vulgar': 664,\n",
       " 'fiction': 665,\n",
       " 'creating': 666,\n",
       " 'budget': 667,\n",
       " 'weight': 668,\n",
       " 'gem': 669,\n",
       " 'universal': 670,\n",
       " 'fan': 671,\n",
       " 'conclusion': 672,\n",
       " 'chase': 673,\n",
       " 'save': 674,\n",
       " 'document': 675,\n",
       " 'predecessor': 676,\n",
       " 'plotting': 677,\n",
       " 'expected': 678,\n",
       " 'basic': 679,\n",
       " 'moviegoers': 680,\n",
       " 'national': 681,\n",
       " 'mainstream': 682,\n",
       " 'mesmerizing': 683,\n",
       " '10': 684,\n",
       " 'screenwriter': 685,\n",
       " 'addition': 686,\n",
       " 'favor': 687,\n",
       " 'stock': 688,\n",
       " 'provide': 689,\n",
       " 'strength': 690,\n",
       " 'college': 691,\n",
       " 'appears': 692,\n",
       " 'hot': 693,\n",
       " 'annoying': 694,\n",
       " 'sheer': 695,\n",
       " 'achieves': 696,\n",
       " 'simplistic': 697,\n",
       " 'release': 698,\n",
       " 'trick': 699,\n",
       " 'brain': 700,\n",
       " 'general': 701,\n",
       " 'commercial': 702,\n",
       " 'virtually': 703,\n",
       " 'soderbergh': 704,\n",
       " 'commentary': 705,\n",
       " 'gripping': 706,\n",
       " 'washington': 707,\n",
       " 'endearing': 708,\n",
       " '20': 709,\n",
       " 'notice': 710,\n",
       " 'father': 711,\n",
       " 'crowd': 712,\n",
       " 'undeniably': 713,\n",
       " 'seagal': 714,\n",
       " 'emerges': 715,\n",
       " 'frank': 716,\n",
       " 'brought': 717,\n",
       " 'bite': 718,\n",
       " 'machine': 719,\n",
       " 'beat': 720,\n",
       " 'protagonist': 721,\n",
       " 'served': 722,\n",
       " 'grief': 723,\n",
       " 'originality': 724,\n",
       " 'unlike': 725,\n",
       " 'century': 726,\n",
       " 'striking': 727,\n",
       " 'treat': 728,\n",
       " 'vampire': 729,\n",
       " 'water': 730,\n",
       " 'identity': 731,\n",
       " 'intentions': 732,\n",
       " 'bits': 733,\n",
       " 'pass': 734,\n",
       " 'manhattan': 735,\n",
       " 'slightly': 736,\n",
       " 'season': 737,\n",
       " 'male': 738,\n",
       " 'desire': 739,\n",
       " 'metaphor': 740,\n",
       " 'awkward': 741,\n",
       " 'skill': 742,\n",
       " 'went': 743,\n",
       " 'wonderfully': 744,\n",
       " 'giving': 745,\n",
       " 'pull': 746,\n",
       " 'mother': 747,\n",
       " 'ghost': 748,\n",
       " 'smile': 749,\n",
       " 'storyline': 750,\n",
       " 'assured': 751,\n",
       " 'longer': 752,\n",
       " 'filmed': 753,\n",
       " 'schneider': 754,\n",
       " 'philosophical': 755,\n",
       " 'worked': 756,\n",
       " 'wasted': 757,\n",
       " 'footage': 758,\n",
       " 'delivery': 759,\n",
       " 'apparently': 760,\n",
       " 'twice': 761,\n",
       " 'blood': 762,\n",
       " 'charisma': 763,\n",
       " 'cinematography': 764,\n",
       " 'large': 765,\n",
       " 'energetic': 766,\n",
       " 'noir': 767,\n",
       " 'christmas': 768,\n",
       " 'harvard': 769,\n",
       " 'combination': 770,\n",
       " 'intrigue': 771,\n",
       " 'dream': 772,\n",
       " 'joy': 773,\n",
       " 'chance': 774,\n",
       " 'indie': 775,\n",
       " 'structure': 776,\n",
       " 'poetry': 777,\n",
       " 'soon': 778,\n",
       " 'jones': 779,\n",
       " 'refreshing': 780,\n",
       " 'subjects': 781,\n",
       " 'attraction': 782,\n",
       " 'eventually': 783,\n",
       " 'heartfelt': 784,\n",
       " 'mixed': 785,\n",
       " 'room': 786,\n",
       " 'lee': 787,\n",
       " 'hold': 788,\n",
       " 'society': 789,\n",
       " 'consistently': 790,\n",
       " 'martin': 791,\n",
       " 'product': 792,\n",
       " 'fit': 793,\n",
       " 'b': 794,\n",
       " 'details': 795,\n",
       " 'growing': 796,\n",
       " 'drag': 797,\n",
       " 'stale': 798,\n",
       " 'amateurish': 799,\n",
       " 'miss': 800,\n",
       " 'gross-out': 801,\n",
       " 'pointless': 802,\n",
       " 'happen': 803,\n",
       " 'weak': 804,\n",
       " 'bond': 805,\n",
       " 'conviction': 806,\n",
       " 'routine': 807,\n",
       " 'possibly': 808,\n",
       " 'business': 809,\n",
       " 'greatest': 810,\n",
       " 'comedic': 811,\n",
       " 'horrible': 812,\n",
       " 'questions': 813,\n",
       " 'dreary': 814,\n",
       " 'value': 815,\n",
       " 'conventional': 816,\n",
       " 'south': 817,\n",
       " 'sea': 818,\n",
       " 'nicely': 819,\n",
       " 'empathy': 820,\n",
       " 'seriously': 821,\n",
       " 'ordinary': 822,\n",
       " 'offensive': 823,\n",
       " 'figure': 824,\n",
       " 'credit': 825,\n",
       " 'barbershop': 826,\n",
       " 'runs': 827,\n",
       " 'body': 828,\n",
       " 'near': 829,\n",
       " 'intensity': 830,\n",
       " 'cute': 831,\n",
       " 'friday': 832,\n",
       " 'directorial': 833,\n",
       " 'raw': 834,\n",
       " 'quietly': 835,\n",
       " 'top': 836,\n",
       " 'wonder': 837,\n",
       " 'older': 838,\n",
       " 'ages': 839,\n",
       " 'overly': 840,\n",
       " 'nowhere': 841,\n",
       " 'stay': 842,\n",
       " 'vividly': 843,\n",
       " 'opening': 844,\n",
       " 'vivid': 845,\n",
       " 'spend': 846,\n",
       " 'adam': 847,\n",
       " 'sum': 848,\n",
       " 'showtime': 849,\n",
       " 'conflict': 850,\n",
       " 'balance': 851,\n",
       " 'george': 852,\n",
       " 'ms.': 853,\n",
       " 'insightful': 854,\n",
       " 'challenging': 855,\n",
       " 'knowing': 856,\n",
       " 'faith': 857,\n",
       " 'performers': 858,\n",
       " 'remarkably': 859,\n",
       " 'late': 860,\n",
       " 'generation': 861,\n",
       " 'disguise': 862,\n",
       " 'waiting': 863,\n",
       " 'car': 864,\n",
       " 'devoid': 865,\n",
       " 'animal': 866,\n",
       " 'ground': 867,\n",
       " 'wait': 868,\n",
       " 'confusing': 869,\n",
       " 'festival': 870,\n",
       " 'produced': 871,\n",
       " 'aside': 872,\n",
       " 'jack': 873,\n",
       " 'move': 874,\n",
       " 'fantastic': 875,\n",
       " 'mayhem': 876,\n",
       " 'presence': 877,\n",
       " 'complicated': 878,\n",
       " 'sight': 879,\n",
       " 'shame': 880,\n",
       " 'fisher': 881,\n",
       " 'frequently': 882,\n",
       " 'vehicle': 883,\n",
       " 'represents': 884,\n",
       " 'presents': 885,\n",
       " 'complete': 886,\n",
       " 'wildly': 887,\n",
       " 'chilling': 888,\n",
       " 'match': 889,\n",
       " 'exhilarating': 890,\n",
       " 'derivative': 891,\n",
       " 'failure': 892,\n",
       " 'damned': 893,\n",
       " 'terms': 894,\n",
       " 'unexpected': 895,\n",
       " 'couple': 896,\n",
       " 'attempts': 897,\n",
       " 'largely': 898,\n",
       " 'tension': 899,\n",
       " 'fears': 900,\n",
       " 'absorbing': 901,\n",
       " 'violent': 902,\n",
       " 'lazy': 903,\n",
       " 'mention': 904,\n",
       " 'empty': 905,\n",
       " 'masterpiece': 906,\n",
       " 'paid': 907,\n",
       " 'involved': 908,\n",
       " 'editing': 909,\n",
       " 'amazing': 910,\n",
       " 'living': 911,\n",
       " 'sentimentality': 912,\n",
       " 'humanity': 913,\n",
       " 'eight': 914,\n",
       " 'form': 915,\n",
       " 'x': 916,\n",
       " 'intended': 917,\n",
       " 'depressing': 918,\n",
       " 'values': 919,\n",
       " 'strangely': 920,\n",
       " 'atmosphere': 921,\n",
       " 'genuinely': 922,\n",
       " 'franchise': 923,\n",
       " 'chris': 924,\n",
       " 'meditation': 925,\n",
       " 'seat': 926,\n",
       " 'guilty': 927,\n",
       " 'forced': 928,\n",
       " 'surface': 929,\n",
       " 'realistic': 930,\n",
       " 'sloppy': 931,\n",
       " 'further': 932,\n",
       " 'loses': 933,\n",
       " 'effect': 934,\n",
       " 'minor': 935,\n",
       " 'television': 936,\n",
       " 'doing': 937,\n",
       " 'ago': 938,\n",
       " 'delight': 939,\n",
       " 'edge': 940,\n",
       " 'mix': 941,\n",
       " 'marks': 942,\n",
       " 'step': 943,\n",
       " 'amount': 944,\n",
       " 'cultural': 945,\n",
       " 'somewhere': 946,\n",
       " 'spectacle': 947,\n",
       " 'wanted': 948,\n",
       " 'pleasant': 949,\n",
       " 'average': 950,\n",
       " 'oddly': 951,\n",
       " 'british': 952,\n",
       " 'jackson': 953,\n",
       " 'person': 954,\n",
       " 'trip': 955,\n",
       " '&': 956,\n",
       " 'soundtrack': 957,\n",
       " 'alive': 958,\n",
       " 'chemistry': 959,\n",
       " 'bullock': 960,\n",
       " 'industry': 961,\n",
       " 'thrills': 962,\n",
       " 'numbers': 963,\n",
       " 'formulaic': 964,\n",
       " 'visuals': 965,\n",
       " 'talking': 966,\n",
       " 'justice': 967,\n",
       " 'wish': 968,\n",
       " 'signs': 969,\n",
       " 'intense': 970,\n",
       " 'sound': 971,\n",
       " 'diverting': 972,\n",
       " 'opportunity': 973,\n",
       " 'moore': 974,\n",
       " 'thinks': 975,\n",
       " 'class': 976,\n",
       " 'hate': 977,\n",
       " 'setting': 978,\n",
       " 'ice': 979,\n",
       " 'interested': 980,\n",
       " 'williams': 981,\n",
       " 'waste': 982,\n",
       " 'pop': 983,\n",
       " 'relationship': 984,\n",
       " 'usually': 985,\n",
       " 'imax': 986,\n",
       " 'number': 987,\n",
       " 'insight': 988,\n",
       " 'frame': 989,\n",
       " 'lets': 990,\n",
       " 'created': 991,\n",
       " 'mediocre': 992,\n",
       " 'sexual': 993,\n",
       " 'quality': 994,\n",
       " 'huge': 995,\n",
       " 'flicks': 996,\n",
       " 'core': 997,\n",
       " 'remember': 998,\n",
       " 'bears': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoding = np.zeros((7000, 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in train_df.iterrows():\n",
    "    phrase = row['Phrase']\n",
    "    phrase.translate(str.maketrans('', '', string.punctuation))\n",
    "    word = \"\"\n",
    "    for s in phrase:\n",
    "        if s == ' ':\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            if word in vocab:\n",
    "                sentence_encoding[i][vocab[word]]+=1\n",
    "            word = \"\"\n",
    "        else:\n",
    "            word += s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 2000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1/(1 + np.exp(-X))\n",
    "def logreg_loss(Theta, X, Y):\n",
    "    #\n",
    "    # Write a logistic regression cost suitable for use with fmin_l_bfgs\n",
    "    #\n",
    "\n",
    "    #reshape Theta\n",
    "    ThetaR = Theta.reshape(X.shape[0],1)\n",
    "    #print(1/(1+ np.exp(-(ThetaR*(X)))))\n",
    "    #print(X)\n",
    "    #print(ThetaR*(X))\n",
    "    nll = -np.sum(Y.dot(np.log2(sigmoid((Theta.T).dot(X)) + 1e-100)) + (1-Y).dot(np.log2(1 -  sigmoid((Theta.T).dot(X))+ 1e-100)))\n",
    "    grad = X.dot((sigmoid((Theta.T).dot(X)) - Y).T)\n",
    "    #print(grad.T[0])\n",
    "    #reshape grad into the shape of Theta, for fmin_l_bfsgb to work\n",
    "    return nll, grad.reshape(Theta.shape)\n",
    "\n",
    "Theta0 = np.random.normal(size = 2000)\n",
    "\n",
    "#\n",
    "# Call a solver\n",
    "#\n",
    "ThetaOpt = sopt.fmin_l_bfgs_b(lambda Theta: logreg_loss(Theta, sentence_encoding.T, sentiment), np.array(Theta0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.37863122,  0.90223436, -0.48271491, ..., -0.01799274,\n",
       "        0.02643673, -0.01255412])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ThetaOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>130218</td>\n",
       "      <td>7012</td>\n",
       "      <td>I have a new favorite musical -- and I 'm not ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7001</th>\n",
       "      <td>130230</td>\n",
       "      <td>7013</td>\n",
       "      <td>This movie plays like an extended dialogue exe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002</th>\n",
       "      <td>130241</td>\n",
       "      <td>7014</td>\n",
       "      <td>Ninety minutes of Viva Castro !</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7003</th>\n",
       "      <td>130246</td>\n",
       "      <td>7015</td>\n",
       "      <td>An indispensable peek at the art and the agony...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7004</th>\n",
       "      <td>130259</td>\n",
       "      <td>7016</td>\n",
       "      <td>Judging by those standards , ` Scratch ' is a ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8524</th>\n",
       "      <td>155985</td>\n",
       "      <td>8540</td>\n",
       "      <td>... either you 're willing to go with this cla...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8525</th>\n",
       "      <td>155998</td>\n",
       "      <td>8541</td>\n",
       "      <td>Despite these annoyances , the capable Claybur...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8526</th>\n",
       "      <td>156022</td>\n",
       "      <td>8542</td>\n",
       "      <td>-LRB- Tries -RRB- to parody a genre that 's al...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8527</th>\n",
       "      <td>156032</td>\n",
       "      <td>8543</td>\n",
       "      <td>The movie 's downfall is to substitute plot fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8528</th>\n",
       "      <td>156040</td>\n",
       "      <td>8544</td>\n",
       "      <td>The film is darkly atmospheric , with Herrmann...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1529 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PhraseId SentenceId                                             Phrase  \\\n",
       "7000   130218       7012  I have a new favorite musical -- and I 'm not ...   \n",
       "7001   130230       7013  This movie plays like an extended dialogue exe...   \n",
       "7002   130241       7014                    Ninety minutes of Viva Castro !   \n",
       "7003   130246       7015  An indispensable peek at the art and the agony...   \n",
       "7004   130259       7016  Judging by those standards , ` Scratch ' is a ...   \n",
       "...       ...        ...                                                ...   \n",
       "8524   155985       8540  ... either you 're willing to go with this cla...   \n",
       "8525   155998       8541  Despite these annoyances , the capable Claybur...   \n",
       "8526   156022       8542  -LRB- Tries -RRB- to parody a genre that 's al...   \n",
       "8527   156032       8543  The movie 's downfall is to substitute plot fo...   \n",
       "8528   156040       8544  The film is darkly atmospheric , with Herrmann...   \n",
       "\n",
       "     Sentiment  \n",
       "7000         3  \n",
       "7001         0  \n",
       "7002         2  \n",
       "7003         3  \n",
       "7004         3  \n",
       "...        ...  \n",
       "8524         2  \n",
       "8525         2  \n",
       "8526         1  \n",
       "8527         1  \n",
       "8528         2  \n",
       "\n",
       "[1529 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent_encoding = np.zeros((1529, 2000))\n",
    "for i, row in test_df.iterrows():\n",
    "    phrase = row['Phrase']\n",
    "    phrase.translate(str.maketrans('', '', string.punctuation))\n",
    "    word = \"\"\n",
    "    for s in phrase:\n",
    "        if s == ' ':\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            if word in vocab:\n",
    "                test_sent_encoding[i-7000][vocab[word]]+=1\n",
    "            word = \"\"\n",
    "        else:\n",
    "            word += s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiment = np.zeros(1529)\n",
    "for i, row in test_df.iterrows():\n",
    "    test_sentiment[i-7000] = row['Sentiment']/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23140704225813855\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.abs(sigmoid(test_sent_encoding.dot(ThetaOpt))-test_sentiment))) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_prob = np.argsort(ThetaOpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative:\n",
      "\n",
      "incoherent\n",
      "unpleasant\n",
      "devoid\n",
      "stealing\n",
      "poor\n",
      "lacking\n",
      "stupid\n",
      "worst\n",
      "horrible\n",
      "disguise\n",
      "poorly\n",
      "shoot\n",
      "lazy\n",
      "rip-off\n",
      "hands\n",
      "eddie\n",
      "repetitive\n",
      "inept\n",
      "car\n",
      "crush\n",
      "\n",
      "Positive:\n",
      "\n",
      "dazzling\n",
      "assured\n",
      "delightfully\n",
      "follow\n",
      "refreshing\n",
      "masterpiece\n",
      "intoxicating\n",
      "mesmerizing\n",
      "rewarding\n",
      "amazing\n",
      "remarkably\n",
      "chilling\n",
      "remarkable\n",
      "originality\n",
      "vibrant\n",
      "feel-good\n",
      "charmer\n",
      "detail\n",
      "ahead\n",
      "eyes\n"
     ]
    }
   ],
   "source": [
    "print(\"Negative:\\n\")\n",
    "for i in range(20):\n",
    "    print(vocab_list[sorted_prob[i]])\n",
    "print(\"\\nPositive:\\n\")\n",
    "for i in range(20):\n",
    "    print(vocab_list[sorted_prob[1999-i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "negation_words = ['not','neither','never','none','nobody','nor','nothing']\n",
    "emphasion_words = ['very','big','enormous','great','much','lot','absolute','most','complete','pure','total','totally']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_with_neg = np.zeros((7000,2000))\n",
    "for i, row in train_df.iterrows():\n",
    "    phrase = row['Phrase']\n",
    "    phrase.translate(str.maketrans('', '', string.punctuation))\n",
    "    word = \"\"\n",
    "    negation = 1\n",
    "    for s in phrase:\n",
    "        if s == ' ':\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            if word in vocab:\n",
    "                train_words_with_neg[i][vocab[word]]+=1*negation\n",
    "            if word in negations:\n",
    "                negation*=-1\n",
    "            else:\n",
    "                negation = 1\n",
    "            word = \"\"\n",
    "        else:\n",
    "            word += s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta0 = np.random.normal(size = 2000)\n",
    "Theta_with_neg = sopt.fmin_l_bfgs_b(lambda Theta: logreg_loss(Theta, sentence_encoding.T, sentiment), np.array(Theta0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent_encoding_with_neg = np.zeros((1529, 2000))\n",
    "for i, row in test_df.iterrows():\n",
    "    phrase = row['Phrase']\n",
    "    phrase.translate(str.maketrans('', '', string.punctuation))\n",
    "    word = \"\"\n",
    "    negation = 1\n",
    "    for s in phrase:\n",
    "        if s == ' ':\n",
    "            if word == \"\":\n",
    "                continue\n",
    "            if word in vocab:\n",
    "                test_sent_encoding_with_neg[i-7000][vocab[word]]+=1*negation\n",
    "            if word in negations:\n",
    "                negation*=-1\n",
    "            else:\n",
    "                negation = 1\n",
    "            word = \"\"\n",
    "        else:\n",
    "            word += s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 2. 1. 3.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 1. 2. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(train_words_with_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22921459646143524\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.abs(sigmoid(test_sent_encoding_with_neg.dot(Theta_with_neg))-test_sentiment))) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_encodings(Neg, Emp):\n",
    "    vector_Sw = np.zeros((7000, 2000))\n",
    "    vector_Neg = np.zeros((7000, 2000))\n",
    "    vector_Emp = np.zeros((7000, 2000))\n",
    "    for i, row in train_df.iterrows():\n",
    "        phrase = row['Phrase']\n",
    "        phrase.translate(str.maketrans('', '', string.punctuation))\n",
    "        word = \"\"\n",
    "        negations = 0\n",
    "        emphasis = 0\n",
    "        for s in phrase:\n",
    "            if s == ' ':\n",
    "                if word == \"\":\n",
    "                    continue\n",
    "                if word in vocab:\n",
    "                    vector_Sw[i][vocab[word]] += (Neg**negations) * (Emp**emphasis)\n",
    "                    if negations != 0:\n",
    "                        vector_Neg[i][vocab[word]] += (negations*(Neg**(negations-1))) * (Emp**emphasis)\n",
    "                    if emphasis != 0:\n",
    "                        vector_Emp[i][vocab[word]]+= (Neg**negations) * (emphasis*(Emp**(emphasis-1)))\n",
    "                    if word in negation_words:\n",
    "                        negations += 1\n",
    "                    else:\n",
    "                        negations = 0\n",
    "                    if word in emphasion_words:\n",
    "                        emphasis += 1\n",
    "                    else:\n",
    "                        emphasis = 0\n",
    "                word = \"\"\n",
    "            else:\n",
    "                word += s.lower()\n",
    "    return vector_Sw,vector_Neg,vector_Emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_logreg_loss(Theta, Y):\n",
    "    Theta_word = Theta[:2000]\n",
    "    Neg = Theta[2000]\n",
    "    Emp = Theta[2001]\n",
    "    X_Sw, X_Neg, X_Emp = word_encodings(Neg, Emp)\n",
    "    X_Sw = X_Sw.T\n",
    "    X_Neg = X_Neg\n",
    "    X_Emp = X_Emp\n",
    "    nll = -np.sum(Y.dot(np.log2(sigmoid((Theta_word.T).dot(X_Sw)) + 1e-100)) + (1-Y).dot(np.log2(1 -  sigmoid((Theta_word.T).dot(X_Sw))+ 1e-100)))\n",
    "    grad = X_Sw.dot((sigmoid((Theta_word.T).dot(X_Sw)) - Y).T)\n",
    "    NegativeGrad = X_Neg.dot(Theta_word).T.dot(sigmoid((Theta_word.T).dot(X_Sw))-Y)\n",
    "    EmphasisGrad = X_Emp.dot(Theta_word).T.dot(sigmoid((Theta_word.T).dot(X_Sw))-Y)\n",
    "    grad = np.append(grad, NegativeGrad)\n",
    "    grad = np.append(grad, EmphasisGrad)\n",
    "    return nll, grad.reshape(Theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta0 = np.random.normal(size = 2002)\n",
    "ThetaOpt = sopt.fmin_l_bfgs_b(lambda Theta: custom_logreg_loss(Theta, sentiment), np.array(Theta0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23186943830457066\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.abs(sigmoid(test_sent_encoding.dot(ThetaOpt[:2000])) - test_sentiment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOq9DmD52l9ZwYdGlCpK8hb",
   "include_colab_link": true,
   "name": "Assignment1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
